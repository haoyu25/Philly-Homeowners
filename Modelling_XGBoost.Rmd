
# Load required packages for modelling
```{r, include=FALSE}
#Package installs -------------------------------------------------------------
load.fun <- function(x) { 
  x <- as.character(x) 
  if(isTRUE(x %in% .packages(all.available=TRUE))) { 
    eval(parse(text=paste("require(", x, ")", sep=""))) 
    print(paste(c(x, " : already installed; requiring"), collapse=''))
  } else { 
    #update.packages()
    print(paste(c(x, " : not installed; installing"), collapse=''))
    eval(parse(text=paste("install.packages('", x, "')", sep=""))) 
    print(paste(c(x, " : installed and requiring"), collapse=''))
    eval(parse(text=paste("require(", x, ")", sep=""))) 
  } 
} 

########### Required Packages ###########
packages = c("bayesplot", "lme4","RcppEigen",
             "tidyverse", "tidyr", "AmesHousing", "broom", "caret", "dials", "doParallel", "e1071", "earth",
             "ggrepel", "glmnet", "ipred", "klaR", "kknn", "pROC", "rpart", "randomForest",
             "sessioninfo", "tidymodels","ranger", "recipes", "workflows", "themis","xgboost",
             "sf", "nngeo", "mapview","data.table","ggplot2","corrr","rsample","parsnip","tidymodels")

for(i in seq_along(packages)){
  packge <- as.character(packages[i])
  load.fun(packge)
}

session_info()
# Color palette
colors2 <- c("0" = "#e42524", "1" = "#00ADA9")
colors6 <- c("#e42524", "#ea7357",  "#f0b48b", "#96d2c7", "#41c3b4", "#00ADA9")
```

# Combining all datasets 
```{r}
properties_model<-fread("properties_model.csv")
permits <- fread("Data/permits.csv")
glimpse(properties_model)
```

# Feature overview
Relationships between the selected features were first examined.

## Continous Features
```{r}
data_continous <-properties_model %>%
    dplyr::select(exemption, 
                  geographic_ward, 
                  balance_rate,#balance_total,
                  #avg_market_value,sd_market_value,perc_bdg_exempt,
                  pct_foreign_born,overall_vacancy_rate,limited_english_rate,
                  diversity_index,median_income,median_home_value,owner_occ_rate,owner_count
                  ) %>%
    gather(Variable, value, -exemption) %>%
    ggplot() + 
    geom_density(aes(value, color=as.factor(exemption)), fill = "transparent") + 
    facet_wrap(~Variable, scales = "free") +
    scale_color_manual(values = palette2) +
    labs(title = "Feature associations with Exemption",
         subtitle = "(continous features)")+ theme_minimal()+
  theme(legend.position = "bottom") 

print(data_continous) 

```

## Categorical Features
```{r}
data_categorical<-properties_model %>%
    dplyr::select(exemption, 
                  same_address,is_deep,large_area,
                  rental_license,commercial_license
                  ) %>%
  
    gather(Variable, value, -exemption) %>%
    filter(complete.cases(.))%>%
    count(Variable, value, exemption)%>%
      ggplot(., aes(value, n, fill = as.factor(exemption))) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free")  +
        scale_fill_manual(values = palette2) +
        labs(x="Categories", y="Value",
             title = "Feature associations with Exemption",
             subtitle = "Categorical features") +
  theme_minimal()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 60, hjust = 1,size=6))+
  scale_x_discrete(labels = function(x) str_trunc(x, 10)) 

print(data_categorical)
```

## Correlation Matrix

```{r}
numericVars <- as.data.frame(properties_model) %>%
  dplyr::select(exemption, 
                
                young_owner_rate,senior_owner_rate,family_hh_rate,
                limited_english_rate, median_home_value, diversity_index,mortgage_burden_rate
                ) %>%  
  select_if(is.numeric) %>% 
  na.omit() 

correlation_matrix <- cor(numericVars)


numericVars %>% 
  correlate() %>% 
  autoplot() +
  geom_tile(aes(fill = r), color = "#e9e9e9") +
  geom_text(aes(label = round(r,digits=2)), size = 3) +
  scale_fill_gradient2(low =  "#00ADA9", mid = "white", high =  "#e42524",
                       midpoint = 0, limits = c(-1, 1),
                       breaks = seq(-1, 1, by = 0.2)) +
  labs(title = "Correlation across numeric variables")  # Set plot title
```

```{r}
library(GGally)

vars1<-numericVars%>%dplyr::select(bach_degree_rate,median_income,median_home_value)
ggpairs(vars1, upper = list(continuous = wrap("cor", size = 3)))
```

The final variables were selected for the model as below.
```{r}
variables <- c("geographic_ward", "same_address", 
               "is_deep", "owner_count",
         "rental_license", "commercial_license","balance_rate", 
         "avg_market_value", "sd_market_value", "has_recent_transfer", "latest_document_type",
          "overall_vacancy_rate", "poverty_rate",
         "young_owner_rate","senior_owner_rate","family_hh_rate",
         "limited_english_rate", "median_home_value", "diversity_index",
         "mortgage_burden_rate","owner_occ_rate")

```

```{r}
### Set up Dataset
properties_model<- properties_model%>%
  mutate(property_ID = seq(1:n()))%>%
  dplyr::select(all_of(variables), exemption, zip_code)

```

The dataset was split into a training and test set, where the training set comprised 75% of the total properties. The split is stratified by zip code, roughly balancing the zip codes across both sets to ensure minimize unequal geographical representation across both the training and test set. The final test set of 25% of the total properties will be left untouched, and used to evaluate the model after tuning and training are complete.

```{r}
### Initial Split for Training and Test
data_split <- initial_split(properties_model, strata = "zip_code", prop = 0.75)
properties_train <- training(data_split)
properties_test  <- testing(data_split)
```

A Leave-One-Group-Out Cross-Validation (LOGOCV) strategy was then implemented within the training dataset, using geographic ward as the grouping variable. Each fold holds out one geographic ward as the validation set while the model is trained on data from all remaining wards. This process iterates across all wards, where each geographic ward serves as a validation fold once. By accounting for potential spatial autocorrelation, this method helps reduce overfitting and prevents data leakage that could arise from training and testing on geographically proximate observations.
```{r}
### Cross Validation
## LOGOCV on Neighborhood with group_vfold_cv()
cv_splits_geo <- group_vfold_cv(properties_train,  
                                group = "geographic_ward")
print(cv_splits_geo)
```

Feature preprocessing is then conducted—categorical variables are one-hot encoded, predictors with zero variance are removed, and numeric predictors are standardized.
```{r}
# Feature Creation
model_rec <- recipe(exemption ~ ., data = properties_train)  %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
```

An XGBoost was then defined with 200 trees, while the number of variables randomly sampled at each split and the minimum number of observation in a terminal node are tuned with with the help of a hyperparameter grid search.

```{r, include=FALSE}
## Model specifications
XGB_plan <- boost_tree() %>%
  set_args(mtry  = tune()) %>%
  set_args(min_n = tune()) %>%
  #set_args(learn_rate = tune())%>%
  #set_args(tree_depth = tune())%>%
  set_args(trees = 200) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")


# Hyperparameter grid for glmnet (penalization)
xgb_grid <- expand.grid(
  mtry = c(3, 7,10
           ),  
  min_n = c(1, 5, 10
            )
  #, learn_rate = c(0.01, 0.3), 
  #tree_depth = c(3, 5, 7, 10)  
)

# create workflow
xgb_wf <-
  workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(XGB_plan)


# fit model to workflow and calculate metrics
control <- control_resamples(save_pred = TRUE, verbose = TRUE)
metrics <- metric_set(rmse, rsq, mape, smape)
xgb_tuned <- xgb_wf %>%
  tune::tune_grid(.,
                  resamples = cv_splits_geo,
                  grid      = xgb_grid,
                  control   = control,
                  metrics   = metrics)
```

The grid search is conducted with the best model selected based on having the lowest Root Mean Squared Error.
```{r}
## metrics across grid
collect_metrics(xgb_tuned)

## 'Best' by some metric and margin
show_best(xgb_tuned, metric = "rsq", n = 15)

xgb_best_params    <- select_best(xgb_tuned, metric = "rmse"   )

xgb_best_wf    <- finalize_workflow(xgb_wf, xgb_best_params)

# last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.

xgb_val_fit_geo <- xgb_best_wf %>% 
  last_fit(split     = data_split,
           control   = control,
           metrics   = metrics)

```

This chosen model is run on the test set for evaluation.
```{r}
# Pull best hyperparam preds from out-of-fold predictions
xgb_best_OOF_preds <- collect_predictions(xgb_tuned) %>% 
  filter(mtry  == xgb_best_params$mtry[1] & min_n == xgb_best_params$min_n[1])

# collect validation set predictions from last_fit model
xgb_val_pred_geo    <- collect_predictions(xgb_val_fit_geo)
```


```{r}
collect_metrics(xgb_tuned) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = factor(mtry), y = mean, fill = factor(min_n))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "RMSE Across Different Hyperparameter Combinations",
       x = "mtry",
       y = "Mean RMSE",
       fill = "min_n") +
  theme_minimal()
```

The threshold is set at 0.5, where any predictions with a probability of 0.5 or over are categorized as predicted to be eligible for the exemption. This has 
```{r}
xgb_val_pred_geo$exemption.pred <- as.factor(ifelse(xgb_val_pred_geo$.pred >= 0.5, "1", "0") )
xgb_val_pred_geo$exemption<-as.factor(xgb_val_pred_geo$exemption)
```

A confusion matrix of the model results was then produced. Breaking it down, 205,188 positives are true positives, meaning they are properties with an existing homestead exemption and were accurately predicted to be eligible for the exemption. 34,598 properties were wrongly predicted to not have an exemption although they were already enrolled in one. This false negative rate should be minimized. 152,373 properties were accurately predicted to not have an existing exemption. One can observe there are 77,317 false positives. This is the target group of interest, in which our model predicts that they should have an exemption, meaning they should be eligible for one, yet are not currently enrolled.
```{r}
caret::confusionMatrix(xgb_val_pred_geo$exemption.pred, xgb_val_pred_geo$exemption, 
                       positive = "1")
```

```{r}
properties_model<-fread("properties_model.csv")
xgb_full_pred <- xgb_best_wf %>% 
  fit(data = properties_model) %>%
  predict(new_data = properties_model) %>%
  bind_cols(properties_model)

```

```{r}
xgb_full_pred <- xgb_full_pred %>%
  mutate(exemption.pred = ifelse(.pred >= 0.5, "1", "0"))
```

# Spacial correlation

```{r}
properties <- fread("data/opa_properties_public.csv")
```


```{r}
properties_model <- properties_model %>%
  left_join(properties %>% dplyr::select(objectid, shape), by = "objectid")
properties_model_sf <- st_as_sf(properties_model, wkt = "shape", crs = 2272)
```

## KNN

```{r}
library(spdep)
knn_nb <- knn2nb(knearneigh(xgb_full_pred_sf, k = 4, use_kd_tree = FALSE))
```

```{r}
library(spdep)
exemption_data <- as.factor(properties_full_pred_sf$exemption)
join_count_test <- joincount.test(exemption_data, listw = nb2listw(knn_nb, style = "B"))
print(join_count_test)
```

The Join Count Test reveals significant spatial clustering for both properties with and without the homestead exemption. For properties without the exemption (coded as 0), the number of same-value neighbors is significantly higher than expected under random distribution (Z = 74.97, p < 2.2e-16). Similarly, properties with the exemption (coded as 1) also demonstrate strong spatial clustering (Z = 77.48, p < 2.2e-16). These results indicate that exemption status is not randomly distributed across space, but rather exhibits clear spatial dependence. This supports the need to incorporate spatial components into further modeling, such as spatial lag variables or spatial error terms.


## global Morans' I

```{r}
properties_full_pred_sf$residual <- properties_full_pred_sf$exemption- properties_full_pred_sf$.pred
```


```{r}
moran_test <- moran.test(properties_full_pred_sf$residual, listw = nb2listw(knn_nb))
print(moran_test) 
```

We conducted a Moran’s I test to examine spatial autocorrelation in the residuals of our model predicting homestead exemption status. The results indicate a statistically significant positive spatial autocorrelation (Moran’s I = 0.0215, p < 0.0001). This suggests that nearby properties tend to have similar residual values, implying that the model may be missing spatially structured predictors. To improve model performance, spatial features such as coordinates or spatial lags (e.g., the share of neighboring properties with exemptions) could be considered.

## add spatial lag

```{r}
library(spdep)
lw <- nb2listw(knn_nb, style = "W") 
properties_full_pred_sf$lag_exemption <- lag.listw(lw, properties_full_pred_sf$exemption)
```

## Model2

```{r}
variables2 <- c("geographic_ward", "same_address", 
               "is_deep", "owner_count",
         "rental_license", "commercial_license","balance_rate", 
         "avg_market_value", "sd_market_value", "has_recent_transfer", "latest_document_type",
          "overall_vacancy_rate", "poverty_rate",
         "young_owner_rate","senior_owner_rate","family_hh_rate",
         "limited_english_rate", "median_home_value", "diversity_index",
         "mortgage_burden_rate","owner_occ_rate"
         ,"lag_exemption","lat_4326","lon_4326")

```

```{r}
### Set up Dataset
properties_model_2<-properties_full_pred_sf
```

```{r}
fwrite(properties_model_2, "properties_model.csv")
```

```{r}
properties_model_2<-fread("properties_model.csv")
```


```{r}
properties_model_2<- properties_model_2%>%
  mutate(property_ID = seq(1:n()))%>%
  dplyr::select(all_of(variables2), exemption, zip_code)%>%
  st_drop_geometry()

```

```{r}
### Initial Split for Training and Test
data_split <- initial_split(properties_model_2, strata = "zip_code", prop = 0.75)
properties_train <- training(data_split)
properties_test  <- testing(data_split)
```

```{r}
### Cross Validation
## LOGOCV on Neighborhood with group_vfold_cv()
cv_splits_geo <- group_vfold_cv(properties_train,  
                                group = "geographic_ward")
print(cv_splits_geo)
```

Feature preprocessing is then conducted—categorical variables are one-hot encoded, predictors with zero variance are removed, and numeric predictors are standardized.
```{r}
# Feature Creation
model_rec <- recipe(exemption ~ ., data = properties_train)  %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
```

An XGBoost was then defined with 200 trees, while the number of variables randomly sampled at each split and the minimum number of observation in a terminal node are tuned with with the help of a hyperparameter grid search.

```{r, include=FALSE}
## Model specifications
XGB_plan <- boost_tree() %>%
  set_args(mtry  = tune()) %>%
  set_args(min_n = tune()) %>%
  #set_args(learn_rate = tune())%>%
  #set_args(tree_depth = tune())%>%
  set_args(trees = 200) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")


# Hyperparameter grid for glmnet (penalization)
xgb_grid <- expand.grid(
  mtry = c(3#, 7,10
           ),  
  min_n = c(#1, 
            5
            #,10
            )
  #, learn_rate = c(0.01, 0.3), 
  #tree_depth = c(3, 5, 7, 10)  
)

# create workflow
xgb_wf <-
  workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(XGB_plan)


# fit model to workflow and calculate metrics
control <- control_resamples(save_pred = TRUE, verbose = TRUE)
metrics <- metric_set(rmse, rsq, mape, smape)
xgb_tuned <- xgb_wf %>%
  tune::tune_grid(.,
                  resamples = cv_splits_geo,
                  grid      = xgb_grid,
                  control   = control,
                  metrics   = metrics)
```



The grid search is conducted with the best model selected based on having the lowest Root Mean Squared Error.
```{r}
## metrics across grid
collect_metrics(xgb_tuned)

## 'Best' by some metric and margin
show_best(xgb_tuned, metric = "rsq", n = 15)

xgb_best_params    <- select_best(xgb_tuned, metric = "rmse"   )

xgb_best_wf    <- finalize_workflow(xgb_wf, xgb_best_params)

# last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.

xgb_val_fit_geo <- xgb_best_wf %>% 
  last_fit(split     = data_split,
           control   = control,
           metrics   = metrics)

```

This chosen model is run on the test set for evaluation.
```{r}
# Pull best hyperparam preds from out-of-fold predictions
xgb_best_OOF_preds <- collect_predictions(xgb_tuned) %>% 
  filter(mtry  == xgb_best_params$mtry[1] & min_n == xgb_best_params$min_n[1])

# collect validation set predictions from last_fit model
xgb_val_pred_geo    <- collect_predictions(xgb_val_fit_geo)
```


```{r}
collect_metrics(xgb_tuned) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = factor(mtry), y = mean, fill = factor(min_n))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "RMSE Across Different Hyperparameter Combinations",
       x = "mtry",
       y = "Mean RMSE",
       fill = "min_n") +
  theme_minimal()
```

```{r}
xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best_params)
# 拟合最终模型以提取变量重要性
xgb_final_fit <- fit(xgb_final_wf, data = properties_train)

# 提取 parsnip 内部的 xgboost 模型
xgb_model <- extract_fit_parsnip(xgb_final_fit)$fit

# 提取变量重要性
xgb_imp <- xgboost::xgb.importance(model = xgb_model)
```

```{r}
# 查看前几项变量重要性（Gain 值越高越重要）
xgb_imp_sorted <- xgb_imp %>% 
  arrange(desc(Gain))

# 打印所有变量及对应 Gain 值
print(xgb_imp_sorted)

# 可视化前 20 个变量的影响力
xgboost::xgb.plot.importance(xgb_imp, top_n = 80, main = "Top 80 Most Influential Variables")

xgb_imp <- xgboost::xgb.importance(model = xgb_model)

# 加一列累计贡献
xgb_imp <- xgb_imp %>%
  arrange(desc(Gain)) %>%
  mutate(cum_gain = cumsum(Gain) / sum(Gain))

# 选出贡献占比前90%的变量
important_vars <- xgb_imp %>% filter(cum_gain <= 0.95)

print(important_vars)
```


The threshold is set at 0.5, where any predictions with a probability of 0.5 or over are categorized as predicted to be eligible for the exemption. This has 
```{r}
xgb_val_pred_geo$exemption.pred <- as.factor(ifelse(xgb_val_pred_geo$.pred >= 0.5, "1", "0") )
xgb_val_pred_geo$exemption<-as.factor(xgb_val_pred_geo$exemption)
```

A confusion matrix of the model results was then produced. Breaking it down, 205,188 positives are true positives, meaning they are properties with an existing homestead exemption and were accurately predicted to be eligible for the exemption. 34,598 properties were wrongly predicted to not have an exemption although they were already enrolled in one. This false negative rate should be minimized. 152,373 properties were accurately predicted to not have an existing exemption. One can observe there are 77,317 false positives. This is the target group of interest, in which our model predicts that they should have an exemption, meaning they should be eligible for one, yet are not currently enrolled.
```{r}
caret::confusionMatrix(xgb_val_pred_geo$exemption.pred, xgb_val_pred_geo$exemption, 
                       positive = "1")
```

```{r}
library(data.table)
fwrite(xgb_val_pred_geo, "xgb_val_pred_geo.csv")
```

```{r}
library(plotROC)

ggplot(xgb_val_pred_geo, aes(d = as.numeric(exemption), m = .pred)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#BF4146") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1, color = '#6683a9') +
  labs(title = "ROC Curve") +
  theme_minimal()
```


The density distribution of the predicted probabilities were then plotted. 
```{r}

ggplot(xgb_val_pred_geo, aes(x = .pred, fill = as.factor(exemption))) + 
  geom_density() +
  facet_grid(exemption ~ .) +
  scale_fill_manual(values = palette2) +
  labs(x = "Probabilities", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome",
             subtitle = "No gentrification = 0, Gentrification = 1") +
  theme_minimal() +
  theme(legend.position = "none") 

```

```{r}
properties_model_2<-properties_full_pred_sf%>%
  rename(.pred1=.pred,exemption.pred1=exemption.pred,residual1=residual)
```

```{r}
xgb_full_pred <- xgb_best_wf %>% 
  fit(data = properties_model_2) %>%
  predict(new_data = properties_model_2) %>%
  bind_cols(properties_model_2)
```

```{r}
xgb_full_pred <- xgb_full_pred %>%
  mutate(exemption.pred2 = ifelse(.pred >= 0.5, "1", "0"))%>%
  rename(.pred2=.pred)
```

```{r}

ggplot(xgb_full_pred, aes(x = .pred2, fill = as.factor(exemption))) + 
  geom_density() +
  facet_grid(exemption ~ .) +
  scale_fill_manual(values = palette2) +
  labs(x = "Probabilities", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome",
             subtitle = "No gentrification = 0, Gentrification = 1") +
  theme_minimal() +
  theme(legend.position = "none") 

```

```{r}
get_fnr <- function(pred, truth) {
  cm <- table(truth = truth, pred = pred)
  fn <- cm["1", "0"]
  tp <- cm["1", "1"]
  fnr <- fn / (fn + tp)
  return(fnr)
}

fnr_model1 <- get_fnr(xgb_full_pred$exemption.pred1, xgb_full_pred$exemption)
fnr_model2 <- get_fnr(xgb_full_pred$exemption.pred2, xgb_full_pred$exemption)

cat("Model1 FNR：", fnr_model1, "\n")
cat("Model2 FNR：", fnr_model2, "\n")
```

```{r}
xgb_full_pred_long <- xgb_full_pred %>%
  pivot_longer(cols = c(.pred1, .pred2),
               names_to = "model",
               values_to = "pred_prob")

ggplot(xgb_full_pred_long, aes(x = pred_prob, fill = as.factor(exemption))) + 
  geom_density(alpha = 0.6) +
  facet_grid(exemption ~ model) +
  scale_fill_manual(values = palette2) +
  labs(x = "Predicted Probability", y = "Density",
       title = "Probability Distribution by Model and Outcome",
       subtitle = "Model A (.pred1) vs Model B (.pred2)") +
  theme_minimal() +
  theme(legend.position = "none")
```


```{r}
fwrite(xgb_full_pred, "xgb_full_pred.csv")
```


## Model3

```{r}
properties_model_3<-fread("properties_model.csv")
```

```{r}
properties_model_3 <- properties_model_3 %>%
  mutate(latest_document_type_DEED = case_when(
    latest_document_type == "DEED" ~ "DEED",
    latest_document_type == "none" ~ "none",
    TRUE ~ "others"
  ))
```

```{r}
variables3 <- c("same_address","rental_license",
                "avg_market_value","sd_market_value","lag_exemption","balance_rate","senior_owner_rate","latest_document_type_DEED","median_home_value", "lon_4326","owner_occ_rate","has_recent_transfer","geographic_ward", "poverty_rate")

```


```{r}
properties_model_3<- properties_model_3%>%
  mutate(property_ID = seq(1:n()))%>%
  dplyr::select(all_of(variables3), exemption, zip_code)%>%
  st_drop_geometry()

```

```{r}
### Initial Split for Training and Test
data_split <- initial_split(properties_model_3, strata = "zip_code", prop = 0.75)
properties_train <- training(data_split)
properties_test  <- testing(data_split)
```

```{r}
### Cross Validation
## LOGOCV on Neighborhood with group_vfold_cv()
cv_splits_geo <- group_vfold_cv(properties_train,  
                                group = "geographic_ward")
print(cv_splits_geo)
```

```{r}
# Feature Creation
model_rec <- recipe(exemption ~ ., data = properties_train)  %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
```

```{r, include=FALSE}
## Model specifications
XGB_plan <- boost_tree() %>%
  set_args(mtry  = tune()) %>%
  set_args(min_n = tune()) %>%
  #set_args(learn_rate = tune())%>%
  #set_args(tree_depth = tune())%>%
  set_args(trees = 200) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") #"classification"


# Hyperparameter grid for glmnet (penalization)
xgb_grid <- expand.grid(
  mtry = c(3, 7,10),  
  min_n = c(1,5,10)
  #, learn_rate = c(0.01, 0.3), 
  #tree_depth = c(3, 5, 7, 10)  
)

# create workflow
xgb_wf <-
  workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(XGB_plan)


# fit model to workflow and calculate metrics
control <- control_resamples(save_pred = TRUE, verbose = TRUE)
metrics <- metric_set(rmse, rsq, mae) #accuracy, sensitivity, specificity
xgb_tuned <- xgb_wf %>%
  tune::tune_grid(.,
                  resamples = cv_splits_geo,
                  grid      = xgb_grid,
                  control   = control,
                  metrics   = metrics)
```



The grid search is conducted with the best model selected based on having the lowest Root Mean Squared Error.
```{r}
## metrics across grid
collect_metrics(xgb_tuned)

## 'Best' by some metric and margin
show_best(xgb_tuned, metric = "rsq", n = 15)

xgb_best_params    <- select_best(xgb_tuned, metric = "rmse"   )

xgb_best_wf    <- finalize_workflow(xgb_wf, xgb_best_params)

# last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.

xgb_val_fit_geo <- xgb_best_wf %>% 
  last_fit(split     = data_split,
           control   = control,
           metrics   = metrics)

```

This chosen model is run on the test set for evaluation.
```{r}
# Pull best hyperparam preds from out-of-fold predictions
xgb_best_OOF_preds <- collect_predictions(xgb_tuned) %>% 
  filter(mtry  == xgb_best_params$mtry[1] & min_n == xgb_best_params$min_n[1])

# collect validation set predictions from last_fit model
xgb_val_pred_geo    <- collect_predictions(xgb_val_fit_geo)
```


```{r}
collect_metrics(xgb_tuned) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = factor(mtry), y = mean, fill = factor(min_n))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "RMSE Across Different Hyperparameter Combinations",
       x = "mtry",
       y = "Mean RMSE",
       fill = "min_n") +
  theme_minimal()
```

```{r}
# Influence of vairables
xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best_params)
xgb_final_fit <- fit(xgb_final_wf, data = properties_train)

xgb_model <- extract_fit_parsnip(xgb_final_fit)$fit

xgb_imp <- xgboost::xgb.importance(model = xgb_model)

xgb_imp_sorted <- xgb_imp %>% 
  arrange(desc(Gain))

print(xgb_imp_sorted)

xgboost::xgb.plot.importance(xgb_imp, top_n = 20, main = "Top 20 Most Influential Variables")

```

```{r}
properties_model<-fread("xgb_full_pred.csv")

properties_model<- properties_model %>%
  mutate(latest_document_type_DEED = case_when(
    latest_document_type == "DEED" ~ "DEED",
    latest_document_type == "none" ~ "none",
    TRUE ~ "others"
  ))
```

```{r}
xgb_full_pred3 <- xgb_best_wf %>% 
  fit(data = properties_model) %>%
  predict(new_data = properties_model) %>%
  bind_cols(properties_model)
```

```{r}
xgb_full_pred3 <- xgb_full_pred3 %>%
  mutate(exemption.pred3 = ifelse(.pred >= 0.5, "1", "0"))%>%
  rename(.pred3=.pred)
```

```{r}

ggplot(xgb_full_pred3, aes(x = .pred3, fill = as.factor(exemption))) + 
  geom_density() +
  facet_grid(exemption ~ .) +
  scale_fill_manual(values = palette2) +
  labs(x = "Probabilities", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome",
             subtitle = "No gentrification = 0, Gentrification = 1") +
  theme_minimal() +
  theme(legend.position = "none") 

```

```{r}
xgb_full_pred3$exemption.pred3 <- factor(as.character(xgb_full_pred3$exemption.pred3), levels = c("0", "1"))
xgb_full_pred3$exemption       <- factor(as.character(xgb_full_pred3$exemption),       levels = c("0", "1"))

caret::confusionMatrix(xgb_full_pred3$exemption.pred3, xgb_full_pred3$exemption, 
                       positive = "1")
```

```{r}
xgb_full_pred3 <- xgb_full_pred3 %>%
  mutate(exemption.pred0.5 = ifelse(.pred3 >= 0.5, "1", "0"),
         exemption.pred0.6 = ifelse(.pred3 >= 0.5, "1", "0"),
         exemption.pred0.7 = ifelse(.pred3 >= 0.5, "1", "0"),
         exemption.pred0.8 = ifelse(.pred3 >= 0.5, "1", "0"),
         exemption.pred0.9 = ifelse(.pred3 >= 0.5, "1", "0"))
```


```{r}
fwrite(xgb_full_pred3, "xgb_full_pred.csv")
```

## Model4

```{r}
properties_model_4<-fread("xgb_full_pred.csv")
```

```{r}
variables4 <- c("same_address","rental_license",
                "avg_market_value","sd_market_value","lag_exemption","balance_rate","senior_owner_rate","latest_document_type_DEED","median_home_value", "lon_4326","owner_occ_rate","has_recent_transfer","geographic_ward", "poverty_rate","lat_4326")

```

```{r}
properties_model_4<- properties_model_4%>%
  mutate(property_ID = seq(1:n()))%>%
  dplyr::select(all_of(variables4), exemption, zip_code)%>%
  st_drop_geometry()

```

```{r}
# 初步划分训练集和测试集
data_split <- initial_split(properties_model_4, strata = "zip_code", prop = 0.75)
properties_train <- training(data_split)
properties_test  <- testing(data_split)
```

To prevent data leakage, spatial lag variables need to be calculated separately for the training and test datasets. If spatial information from the test set is used during the training phase, the model would "see" part of the test data's structure in advance, leading to overfitting and failing to reflect the true generalization ability of the model.

Therefore, in this analysis, we first split the data and then independently calculate the spatial adjacency relationships and spatial lag values for each subset. This approach not only better reflects real-world scenarios (where the model only has access to the training data for prediction) but also improves the accuracy and reliability of model evaluation.

```{r}
library(spdep)
library(dplyr)
library(zoo)

coords_train <- properties_train %>% 
  dplyr::select(lon_4326, lat_4326) %>% 
  data.matrix()

knn_nb_train <- knn2nb(knearneigh(coords_train, k = 4))
lw_train <- nb2listw(knn_nb_train, style = "W")

properties_train$lag_exemption <- lag.listw(lw_train, properties_train$exemption)
# 填补 NA
mean_lag_train <- mean(properties_train$lag_exemption, na.rm = TRUE)
properties_train$lag_exemption[is.na(properties_train$lag_exemption)] <- mean_lag_train

coords_test <- properties_test %>% 
  dplyr::select(lon_4326, lat_4326) %>% 
  data.matrix()

knn_nb_test <- knn2nb(knearneigh(coords_test, k = 4))
lw_test <- nb2listw(knn_nb_test, style = "W")

properties_test$lag_exemption <- lag.listw(lw_test, properties_test$exemption)
# 填补 NA
mean_lag_test <- mean(properties_test$lag_exemption, na.rm = TRUE)
properties_test$lag_exemption[is.na(properties_test$lag_exemption)] <- mean_lag_test
```



```{r}
### Cross Validation
## LOGOCV on Neighborhood with group_vfold_cv()
cv_splits_geo <- group_vfold_cv(properties_train,  
                                group = "geographic_ward")
print(cv_splits_geo)
```

```{r}
# Feature Creation
model_rec <- recipe(exemption ~ ., data = properties_train)  %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
```

```{r, include=FALSE}
## Model specifications
XGB_plan <- boost_tree() %>%
  set_args(mtry  = tune()) %>%
  set_args(min_n = tune()) %>%
  #set_args(learn_rate = tune())%>%
  #set_args(tree_depth = tune())%>%
  set_args(trees = 200) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") #"classification"


# Hyperparameter grid for glmnet (penalization)
xgb_grid <- expand.grid(
  mtry = c(3, 7,10),  
  min_n = c(1,5,10)
  #, learn_rate = c(0.01, 0.3), 
  #tree_depth = c(3, 5, 7, 10)  
)

# create workflow
xgb_wf <-
  workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(XGB_plan)


# fit model to workflow and calculate metrics
control <- control_resamples(save_pred = TRUE, verbose = TRUE)
metrics <- metric_set(rmse, rsq, mae) #accuracy, sensitivity, specificity
xgb_tuned <- xgb_wf %>%
  tune::tune_grid(.,
                  resamples = cv_splits_geo,
                  grid      = xgb_grid,
                  control   = control,
                  metrics   = metrics)
```

The grid search is conducted with the best model selected based on having the lowest Root Mean Squared Error.
```{r}
## metrics across grid
collect_metrics(xgb_tuned)

## 'Best' by some metric and margin
show_best(xgb_tuned, metric = "rsq", n = 15)

xgb_best_params    <- select_best(xgb_tuned, metric = "rmse"   )

xgb_best_wf    <- finalize_workflow(xgb_wf, xgb_best_params)

# last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.

xgb_val_fit_geo <- xgb_best_wf %>% 
  last_fit(split     = data_split,
           control   = control,
           metrics   = metrics)

```

## Model5

```{r}
properties_model_5<-fread("xgb_full_pred.csv")
```

```{r}
variables5 <- c("same_address","rental_license",
                "avg_market_value","sd_market_value","lag_exemption","balance_rate","senior_owner_rate","latest_document_type_DEED","median_home_value", "lon_4326","owner_occ_rate","has_recent_transfer","geographic_ward", "poverty_rate","lat_4326")

```

```{r}
properties_model_5<- properties_model_5%>%
  mutate(property_ID = seq(1:n()))%>%
  dplyr::select(all_of(variables5), exemption, zip_code)%>%
  st_drop_geometry()%>%
  mutate(exemption=as.factor(exemption))

```

```{r}
# 初步划分训练集和测试集
data_split <- initial_split(properties_model_5, strata = "zip_code", prop = 0.75)
properties_train <- training(data_split)
properties_test  <- testing(data_split)
```

To prevent data leakage, spatial lag variables need to be calculated separately for the training and test datasets. If spatial information from the test set is used during the training phase, the model would "see" part of the test data's structure in advance, leading to overfitting and failing to reflect the true generalization ability of the model.

Therefore, in this analysis, we first split the data and then independently calculate the spatial adjacency relationships and spatial lag values for each subset. This approach not only better reflects real-world scenarios (where the model only has access to the training data for prediction) but also improves the accuracy and reliability of model evaluation.

```{r}
library(spdep)
library(dplyr)
library(zoo)

coords_train <- properties_train %>% 
  dplyr::select(lon_4326, lat_4326) %>% 
  data.matrix()

knn_nb_train <- knn2nb(knearneigh(coords_train, k = 4))
lw_train <- nb2listw(knn_nb_train, style = "W")

properties_train$lag_exemption <- lag.listw(lw_train, properties_train$exemption)
# 填补 NA
mean_lag_train <- mean(properties_train$lag_exemption, na.rm = TRUE)
properties_train$lag_exemption[is.na(properties_train$lag_exemption)] <- mean_lag_train

coords_test <- properties_test %>% 
  dplyr::select(lon_4326, lat_4326) %>% 
  data.matrix()

knn_nb_test <- knn2nb(knearneigh(coords_test, k = 4))
lw_test <- nb2listw(knn_nb_test, style = "W")

properties_test$lag_exemption <- lag.listw(lw_test, properties_test$exemption)
# 填补 NA
mean_lag_test <- mean(properties_test$lag_exemption, na.rm = TRUE)
properties_test$lag_exemption[is.na(properties_test$lag_exemption)] <- mean_lag_test
```

```{r}
properties_train <- properties_train %>%
  mutate(exemption = as.factor(exemption))

properties_test <- properties_test %>%
  mutate(exemption = as.factor(exemption))
```




```{r}
### Cross Validation
## LOGOCV on Neighborhood with group_vfold_cv()
cv_splits_geo <- group_vfold_cv(properties_train,  
                                group = "geographic_ward")
print(cv_splits_geo)
```

```{r}

```


```{r}
# Feature Creation
model_rec <- recipe(exemption ~ ., data = properties_train)  %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
```

```{r, include=FALSE}
## Model specifications
XGB_plan <- boost_tree() %>%
  set_args(mtry  = tune()) %>%
  set_args(min_n = tune()) %>%
  #set_args(learn_rate = tune())%>%
  #set_args(tree_depth = tune())%>%
  set_args(trees = 200) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") #"classification"


# Hyperparameter grid for glmnet (penalization)
xgb_grid <- expand.grid(
  mtry = c(3, 7,
           10),  
  min_n = c(1,5,
            10)
  #, learn_rate = c(0.01, 0.3), 
  #tree_depth = c(3, 5, 7, 10)  
)

# create workflow
xgb_wf <-
  workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(XGB_plan)


# fit model to workflow and calculate metrics
control <- control_resamples(save_pred = TRUE, verbose = TRUE)
metrics <- metric_set(roc_auc, accuracy, kap)
xgb_tuned <- xgb_wf %>%
  tune::tune_grid(.,
                  resamples = cv_splits_geo,
                  grid      = xgb_grid,
                  control   = control,
                  metrics   = metrics)
```

The grid search is conducted with the best model selected based on having the lowest Root Mean Squared Error.
```{r}
## metrics across grid
collect_metrics(xgb_tuned)

## 'Best' by some metric and margin
show_best(xgb_tuned, metric = "accuracy", n = 15)

xgb_best_params    <- select_best(xgb_tuned, metric = "accuracy"   )

xgb_best_wf    <- finalize_workflow(xgb_wf, xgb_best_params)

# last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.

xgb_val_fit_geo <- xgb_best_wf %>% 
  last_fit(split     = data_split,
           control   = control,
           metrics   = metrics)
```

This chosen model is run on the test set for evaluation.
```{r}
# Pull best hyperparam preds from out-of-fold predictions
xgb_best_OOF_preds <- collect_predictions(xgb_tuned) %>% 
  filter(mtry  == xgb_best_params$mtry[1] & min_n == xgb_best_params$min_n[1])

# collect validation set predictions from last_fit model
xgb_val_pred_geo    <- collect_predictions(xgb_val_fit_geo)
```


```{r}
collect_metrics(xgb_tuned) %>%
  filter(.metric == "accuracy") %>%
  ggplot(aes(x = factor(mtry), y = mean, fill = factor(min_n))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "RMSE Across Different Hyperparameter Combinations",
       x = "mtry",
       y = "Mean RMSE",
       fill = "min_n") +
  theme_minimal()
```

```{r}
# Influence of vairables
xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best_params)
xgb_final_fit <- fit(xgb_final_wf, data = properties_train)

xgb_model <- extract_fit_parsnip(xgb_final_fit)$fit

xgb_imp <- xgboost::xgb.importance(model = xgb_model)

xgb_imp_sorted <- xgb_imp %>% 
  arrange(desc(Gain))

print(xgb_imp_sorted)

xgboost::xgb.plot.importance(xgb_imp, top_n = 30, main = "Top 20 Most Influential Variables")

```


```{r}
properties_predict<-fread("properties_model.csv")
glimpse(properties_predict)
```

```{r}
## KNN
properties_predict_sf <- st_as_sf(properties_predict, coords = c("x_2272", "y_2272"), crs = 2272)

library(spdep)
knn_nb <- knn2nb(knearneigh(properties_predict_sf, k = 4, use_kd_tree = FALSE))
```

## add spatial lag

```{r}
library(spdep)
lw <- nb2listw(knn_nb, style = "W") 
properties_predict_sf$lag_exemption <- lag.listw(lw, properties_predict_sf$exemption)
```

```{r}
properties_predict0<-properties_predict_sf%>%
  mutate(exemption=as.factor(exemption))%>%
  st_drop_geometry
```

```{r}
properties_predict0<- properties_predict0 %>%
  mutate(latest_document_type_DEED = case_when(
    latest_document_type == "DEED" ~ "DEED",
    latest_document_type == "none" ~ "none",
    TRUE ~ "others"
  ))
```


```{r}
xgb_full_pred <- xgb_best_wf %>% 
  fit(data = properties_predict0) %>%
  predict(new_data = properties_predict0,type="prob") %>%
  bind_cols(properties_predict0)
```

```{r}
xgb_full_pred <- xgb_full_pred %>%
  mutate(exemption.pred = ifelse(.pred_1 >= 0.5, "1", "0"))%>%
  mutate(.pred=.pred_1)
```

```{r}
ggplot(xgb_full_pred, aes(x = .pred, fill = as.factor(exemption))) + 
  geom_density() +
  facet_grid(exemption ~ .) +
  scale_fill_manual(values = colors2) +
  labs(x = "Probabilities", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome",
             subtitle = "No Exemption = 0, Exemption = 1") +
  theme_minimal() +
  theme(legend.position = "none") 
```

```{r}
xgb_full_pred$exemption.pred <- factor(as.character(xgb_full_pred$exemption.pred), levels = c("0", "1"))
xgb_full_pred$exemption       <- factor(as.character(xgb_full_pred$exemption),       levels = c("0", "1"))

caret::confusionMatrix(xgb_full_pred$exemption.pred, xgb_full_pred$exemption, 
                       positive = "1")
```

```{r}
xgb_full_pred <- xgb_full_pred %>%
  mutate(exemption.pred0.5 = ifelse(.pred >= 0.5, "1", "0"),
         exemption.pred0.6 = ifelse(.pred >= 0.6, "1", "0"),
         exemption.pred0.7 = ifelse(.pred >= 0.7, "1", "0"),
         exemption.pred0.8 = ifelse(.pred >= 0.8, "1", "0"),
         exemption.pred0.9 = ifelse(.pred >= 0.9, "1", "0"))
```


```{r}
fwrite(xgb_full_pred, "xgb_full_pred_05056.csv")
```

```{r}
fp_data <- xgb_full_pred %>%
  filter(exemption.pred == "1", exemption == "0")
```

```{r}
fwrite(fp_data, "fp_data_0506.csv")
```

```{r}
tp <- sum(xgb_full_pred$exemption.pred == "1" & xgb_full_pred$exemption == "1")
fp <- sum(xgb_full_pred$exemption.pred == "1" & xgb_full_pred$exemption == "0")

precision <- tp / (tp + fp)
precision
```


```{r}
library(dplyr)
library(tidyr)

resid_summary <- properties_result %>%
  summarise(across(starts_with("residual"), list(
    rmse = ~ sqrt(mean(.^2, na.rm = TRUE)),
    mae = ~ mean(abs(.), na.rm = TRUE),
    meanresid = ~ mean(., na.rm = TRUE)
  ))) %>%
  pivot_longer(cols = everything(), 
               names_to = "metric", 
               values_to = "value") %>%
  extract(metric, into = c("residual", "stat"), 
          regex = "(residual\\d+)_(.*)") %>%
  pivot_wider(names_from = stat, values_from = value) %>%
  dplyr::select(residual, mae, meanresid, rmse)

print(resid_summary)
```

```{r}
sf_data <- properties_result %>%
  st_as_sf(coords = c("lon_4326", "lat_4326"), crs = 4326)
```


```{r}
library(spdep)

coords <- cbind(properties_result$x_2272, properties_result$y_2272)
neighbors <- knn2nb(knearneigh(coords, k = 4))
weights <- nb2listw(neighbors)

moran1 <- moran.test(properties_result$residual1, listw = weights)
moran2 <- moran.test(properties_result$residual2, listw = weights)
moran3 <- moran.test(properties_result$residual3, listw = weights)
moran4 <- moran.test(properties_result$residual4, listw = weights)

```

```{r}
library(broom)
library(dplyr)
library(purrr)  # 用于 map 操作

# 创建一个命名列表
moran_list <- list(
  residual1 = tidy(moran1),
  residual2 = tidy(moran2),
  residual3 = tidy(moran3),
  residual4 = tidy(moran4)
)

# 合并为 data.frame 并选择需要的列
moran_summary <- moran_list %>%
  bind_rows(.id = "residual") %>%
  dplyr::select(residual, moran_i = estimate1, z_stat = statistic, p_value = p.value)

print(moran_summary)

```

Among the four models, only residual1 shows significant positive spatial autocorrelation (Moran’s I = 0.0215, p < 0.001), indicating spatial clustering of residuals and suggesting that the model fails to fully account for spatially structured variables. In contrast, residual2 to residual4 all exhibit slight negative Moran’s I values with extremely high p-values (p = 1), indicating no significant spatial autocorrelation and thus better handling of spatial dependencies.


